% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropy.R
\name{calculate_kl_divergence}
\alias{calculate_kl_divergence}
\title{Calculate Kullback-Leibler Divergence}
\usage{
calculate_kl_divergence(observed, reference, epsilon = 1e-10)
}
\arguments{
\item{observed}{Named numeric vector of observed codon frequencies}

\item{reference}{Named numeric vector of reference codon frequencies}

\item{epsilon}{Small value to avoid log(0) (default: 1e-10)}
}
\value{
Numeric value of KL divergence
}
\description{
Calculate the Kullback-Leibler (KL) divergence between two codon frequency
distributions. This measures how one distribution diverges from a second,
expected distribution.
}
\examples{
sequence1 <- "ATGATGATGTTATTATTACGCCGCCGCC"
sequence2 <- "TTATTATTACGCCGCCGCCATGATGATG"
freqs1 <- calculate_codon_frequencies(sequence1)
freqs2 <- calculate_codon_frequencies(sequence2)
kl_div <- calculate_kl_divergence(freqs1, freqs2)
}
